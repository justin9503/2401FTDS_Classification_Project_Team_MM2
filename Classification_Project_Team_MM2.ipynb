{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c59607",
   "metadata": {},
   "source": [
    "# PACKAGE IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1959fd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "These libraries and tools collectively provide a comprehensive set of capabilities for handling data (pandas, numpy), manipulating text (re, nltk), and performing advanced natural language processing tasks (nltk). They are widely used in data science, machine learning, and text analytics projects due to their efficiency and versatility.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e9bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48b83db",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a3d37-372e-4053-9231-e3e60acc2e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Data/processed/test.csv')\n",
    "test_data = pd.read_csv('Data/processed/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4adf14-552e-40e3-a894-fdc64a4ad85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6435942-1eb4-4b30-bc44-6046caf27107",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b294cd0",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing for Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff17d924",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "This section covers the process of data cleaning, which involves preparing text data for analysis by removing errors and inconsistencies. It includes downloading NLTK packages, loading datasets, and cleaning the text by removing noise, punctuation, and converting to lowercase. The text is then tokenized, stop words are removed, and words are stemmed and lemmatized. Finally, the processed text is reassembled into strings, with an option to save the cleaned datasets to CSV files.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67600e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('Data/processed/test.csv')\n",
    "test_data = pd.read_csv('Data/processed/train.csv')\n",
    "\n",
    "# Clean text: remove noise and punctuation, convert to lower case\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove all punctuations and special characters\n",
    "    return text.strip().lower()\n",
    "\n",
    "train_data['headlines'] = train_data['headlines'].apply(clean_text)\n",
    "train_data['description'] = train_data['description'].apply(clean_text)\n",
    "train_data['content'] = train_data['content'].apply(clean_text)\n",
    "\n",
    "test_data['headlines'] = test_data['headlines'].apply(clean_text)\n",
    "test_data['description'] = test_data['description'].apply(clean_text)\n",
    "test_data['content'] = test_data['content'].apply(clean_text)\n",
    "\n",
    "# Tokenization\n",
    "train_data['headlines'] = train_data['headlines'].apply(word_tokenize)\n",
    "train_data['description'] = train_data['description'].apply(word_tokenize)\n",
    "train_data['content'] = train_data['content'].apply(word_tokenize)\n",
    "\n",
    "test_data['headlines'] = test_data['headlines'].apply(word_tokenize)\n",
    "test_data['description'] = test_data['description'].apply(word_tokenize)\n",
    "test_data['content'] = test_data['content'].apply(word_tokenize)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "train_data['headlines'] = train_data['headlines'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "train_data['description'] = train_data['description'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "train_data['content'] = train_data['content'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "test_data['headlines'] = test_data['headlines'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "test_data['description'] = test_data['description'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "test_data['content'] = test_data['content'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "train_data['headlines'] = train_data['headlines'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "train_data['description'] = train_data['description'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "train_data['content'] = train_data['content'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "test_data['headlines'] = test_data['headlines'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "test_data['description'] = test_data['description'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "test_data['content'] = test_data['content'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "train_data['headlines'] = train_data['headlines'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "train_data['description'] = train_data['description'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "train_data['content'] = train_data['content'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "test_data['headlines'] = test_data['headlines'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "test_data['description'] = test_data['description'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "test_data['content'] = test_data['content'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# Convert lists back to strings\n",
    "train_data['headlines'] = train_data['headlines'].apply(lambda x: ' '.join(x))\n",
    "train_data['description'] = train_data['description'].apply(lambda x: ' '.join(x))\n",
    "train_data['content'] = train_data['content'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "test_data['headlines'] = test_data['headlines'].apply(lambda x: ' '.join(x))\n",
    "test_data['description'] = test_data['description'].apply(lambda x: ' '.join(x))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Save the cleaned datasets (optional)\n",
    "train_data.to_csv('train_cleaned.csv', index=False)\n",
    "test_data.to_csv('test_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61542dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc1b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff37b3",
   "metadata": {},
   "source": [
    "### Removing noise\n",
    "\n",
    "removing unnecessary information to get the data into a usable format. The code remove the following.\n",
    "- Remove the web-url for train_data.\n",
    "- Remove the web-url for test_data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d3b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pattern_url =  r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "url = r'url-web'\n",
    "train_data['url'] = train_data['url'].replace(to_replace = pattern_url, value = url, regex = True)\n",
    "test_data['url'] = test_data['url'].replace(to_replace = pattern_url, value = url, regex = True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9976e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c22b80",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
